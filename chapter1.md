# 第1章 強化学習の基礎

## 1.1 強化学習とは
強化学習は、エージェントが環境と試行錯誤を繰り返しながら報酬を最大化する方法を学ぶ枠組みだ。教師あり学習のように正解ラベルを用意せず、エージェントが自ら経験から行動を改善する点が特徴である。既に学んだ確率や関数の知識を活用しつつ、行動の良さを数値化して学習を進める。

### 1.1.1 エージェントと環境
エージェントは行動を選ぶ主体で、環境はその結果を受けて状態を更新する。エージェントは現在の状態を観測し、それに応じて行動を選択する。環境は行動に応じた新たな状態と報酬を返し、これをもとにエージェントは次の行動を決める。

### 1.1.2 状態・行動・報酬
状態は環境の持つ情報を表し、行動はエージェントが選べる選択肢を指す。報酬は行動の結果として与えられる数値で、エージェントはこの報酬の累積を最大化するよう振る舞う。割引率 \(\gamma\) を用いると、時刻 \(t\) で得られる総報酬は次の式で表せる。
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]
ここで \(0 \leq \gamma \leq 1\) とすると、将来の報酬がどれだけ重要かを調整できる。

## 1.2 学習の流れ
エージェントは状態を観測して行動を選択し、環境から報酬と次の状態を受け取る。このサイクルを繰り返しながら、得られたデータをもとに方策や価値関数を更新していく。学習が進むにつれ、エージェントはより高い報酬を得る行動を選べるようになる。

### 1.2.1 探索と活用
初期段階では環境についての知識が乏しいため、さまざまな行動を試す探索が重要だ。一方で、既に高い報酬を得られる行動が分かっている場合は、それを活用することで効率的に報酬を得られる。探索と活用のバランスを取ることが、学習を安定させる鍵となる。

### 1.2.2 学習と評価
行動の結果得られたデータから、エージェントは方策や価値関数を更新する。更新後の方策を用いて再び環境で振る舞い、得られた報酬を評価することで学習の進み具合を確認できる。評価を通じて方策を改善し、より高い累積報酬を目指す。
