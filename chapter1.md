# 第1章 強化学習の基礎

## 1.1 強化学習とは
強化学習は、エージェントが環境と相互作用しながら最適な行動を学ぶ枠組みだ。教師あり学習のように正解ラベルが存在しないため、エージェントは試行錯誤によって経験を積み、報酬を最大化する方策を見つけ出す。既に学んだ確率論や関数の知識を活かしつつ、行動の価値を定量的に評価して学習を進める点が特徴である。
教師あり学習や教師なし学習ではデータから直接パターンを抽出するが、強化学習では環境とのインタラクションを通じて経験を蓄積し、方策を改善していく。この違いにより、強化学習は連続的な意思決定が求められる問題に特に適している。

強化学習では短期的な報酬だけでなく、長期的に得られる報酬の総和を重視する。たとえば迷路を解く際、すぐに得られる小さな報酬よりも、最終的にゴールへ到達することで得られる大きな報酬を優先すべき場合が多い。このように現在の選択が将来にどのような影響を与えるかを考慮しながら、行動を選択する点が強化学習の鍵となる。

また、強化学習はゲームAIやロボット制御など、実世界での意思決定を要する分野で広く利用されている。環境からのフィードバックを活用して方策を改善するため、複雑な環境下でも自律的に行動を学習できるという利点がある。これから学ぶ手法は、こうした応用にもつながる基礎となる。
さらに、強化学習は他の学習手法と密接に関わっている。例えば動的計画法では、環境の完全なモデルが分かっていると仮定し、最適な方策を解析的に求める。一方、強化学習では環境モデルが未知の場合でも、シミュレーションや試行錯誤から経験を得て学習を進める点が異なる。これにより、現実的な複雑な問題にも対応しやすくなる。

### 1.1.1 エージェントと環境
エージェントは行動を選ぶ主体であり、環境はその行動に応じた結果を返す存在だ。エージェントは現在の状態を観測し、観測した情報をもとに行動を決定する。環境はエージェントの行動を受け取り、新たな状態と報酬を生成してエージェントに与える。この一連のやり取りが、強化学習における学習サイクルとなる。

環境の例としては、チェスの盤面やロボットが動く物理空間などが挙げられる。エージェントはこれらの環境を通じて経験を積み、より良い行動を学んでいく。環境が返す状態には、時としてノイズや不確実性が含まれる場合がある。そのためエージェントは完全な情報を得られない状況でも、最善の行動を選ぶ必要がある。

エージェントが利用できる情報は、観測可能な状態だけに限られることが多い。たとえば部分観測マルコフ過程では、環境の全体像を直接知ることはできない。こうした状況では、過去の経験を活かして隠れた状態を推測する工夫が求められる。このようにエージェントと環境の関係を理解することが、強化学習の第一歩となる。
エージェントを設計する際には、状態の取得方法や行動の出力形式を慎重に定める必要がある。センサーから得られる観測値にノイズが含まれる場合、フィルタリングや前処理を行うことで学習を安定させることができる。また、環境とのインタフェースを明確にすることで、学習アルゴリズムを実装しやすくなる。
エージェントと環境の設計方針をきちんと定めることが、実践的な強化学習システムの構築には欠かせない。

### 1.1.2 状態・行動・報酬
状態は環境が持つ情報を表し、エージェントは観測した状態をもとに行動を選択する。状態は離散的な値で表現される場合もあれば、ロボットの位置や速度のように連続値で表されることもある。状態の表現方法によって、エージェントが取りうる行動や学習方法も変化する。

行動とは、エージェントが環境に対して起こす操作や選択肢を指す。行動空間が離散的な場合、各行動を試しながら最適なものを選ぶことになる。一方、連続的な行動空間では、行動を微調整しながら報酬を高める方法を学習する必要がある。どちらのケースでも、行動が環境に与える影響を数値化して評価することが重要だ。

報酬は、エージェントの行動がどれだけ望ましいかを示す指標として機能する。一般には、行動後に環境から返される実数値で表され、報酬が高いほどエージェントの目標に近づくとみなされる。割引率 \(\gamma\) を導入すると、時刻 \(t\) における総報酬 \(G_t\) は次の式で表される。
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]
ここで \(0 \leq \gamma \leq 1\) と定めると、将来の報酬がどの程度重要かを調整できる。割引率が1に近いほど、遠い将来の報酬を重視することになる。逆に割引率が小さい場合、目先の報酬を優先する方策となる。

報酬設計は強化学習の成功を左右する重要な要素だ。報酬が極端に偏っていると、エージェントは望ましくない行動を学習してしまうことがある。適切な報酬を与えつつ、エージェントが学習しやすい形で環境を設計することが実践上の課題となる。
報酬設計を工夫することで、学習を効率化できることが知られている。たとえば、最終目標を達成したときだけ大きな報酬を与えるのではなく、途中経過にも小さな報酬を配置することで学習の指針を与える方法を報酬整形と呼ぶ。
報酬整形を用いると、エージェントはゴールから遠い状態でも何を目指せばよいかを理解しやすくなるが、誤った報酬を設計すると本来の目標から外れた行動を学習する危険もある。
強化学習の理論的基盤にはマルコフ決定過程(MDP)がある。MDPでは、次の状態と報酬が現在の状態と行動のみに依存すると仮定する。この性質をマルコフ性と呼び、問題を数学的に扱いやすくする重要な前提となる。
MDPの枠組みによって、将来の行動を予測する際に過去の状態をすべて覚える必要がなくなり、状態遷移確率や報酬関数を用いた解析が可能になる。

## 1.2 学習の流れ
学習は、エージェントが状態を観測し、行動を選択し、環境から報酬と次の状態を受け取るというサイクルで進む。これを1ステップとし、このサイクルを繰り返すことでエージェントは環境の特徴を理解し始める。経験を蓄積することで方策や価値関数を更新し、報酬をより多く得られる行動へと徐々に改善していく。
学習サイクルの一般的な流れは次のとおりだ。
1. 状態を観測する
2. 方策に従って行動を選ぶ
3. 行動を環境に適用する
4. 報酬と次の状態を受け取る
5. 得られた情報で方策や価値関数を更新する
6. 次のステップへ進む
このループを繰り返すことで、エージェントは経験を蓄積し、より良い行動を学習する。

探索と活用を理論的に分析する枠組みとして、多腕バンディット理論が知られている。ここでは後悔最小化と呼ばれる指標を用いて、どれだけ効率的に最良の行動を発見できるかを評価する。後悔を抑える戦略は、未知の環境に対しても安定した性能を保証する上で重要な考え方となる。
学習の過程では、方策 \(\pi\) を更新する手法や、状態価値関数 \(V(s)\) と行動価値関数 \(Q(s,a)\) を求める手法が利用される。方策とは状態から行動を決定する確率分布であり、価値関数はその方策に従ったときに期待される報酬の総和を表す。価値関数を高めるよう方策を更新することで、エージェントはより望ましい行動を取れるようになる。
強化学習の代表的なアルゴリズムとして、価値ベースのQ学習やSARSA、方策ベースのポリシーグラディエント法などがある。これらは問題の性質に応じて使い分けられ、値関数と方策を同時に更新するアクター・クリティック法ではそれぞれの長所を兼ね備えている。
(arepsilon)-greedy法では、学習が進むにつれて(arepsilon)を減少させる(arepsilon)-decayがよく使われる。初期は探索を重視し、十分なデータが集まった後は活用を中心に据えることで、学習効率と最終性能のバランスを取ることができる。
さらに、近年ではディープラーニングを組み合わせた深層強化学習が注目され、大規模で複雑な環境でも高い性能を示す事例が増えている。

エージェントは一般にエピソードと呼ばれる単位で学習を進める。エピソードは初期状態から開始し、終了条件に達するまでの一連のステップを指す。エピソードの総報酬を評価することで、学習の進み具合を確認できる。複数エピソードを通じて統計的に安定した結果を得ることが、実用的な学習では欠かせない。

### 1.2.1 探索と活用
初期段階では環境についての知識が乏しいため、さまざまな行動を試す探索が重要だ。探索を通じて得られる情報が増えるほど、エージェントはより正確に環境を把握できる。一方で、既に高い報酬を得られる行動が分かっているなら、それを積極的に活用して報酬を稼ぐことも必要となる。
探索と活用の問題は、バンディット問題としても知られている。バンディット問題では、複数の選択肢から報酬の高いものを見つけるために、試行を重ねながら判断を行う。強化学習の初期研究ではこの単純な設定がよく使われ、ここで得られた知見が複雑な環境にも応用されている。
もう一つの代表的手法にUCB法がある。UCBでは、行動価値の推定値に不確実さを示す項を加算し、推定値が高い行動とまだ十分に試していない行動の双方を選択する。これにより、長期的に大きな報酬を得られる行動を効率的に見つけ出すことが可能になる。
近年では、探索と活用のバランスを自動的に調整するアルゴリズムも研究されている。エージェントの経験に基づいて探索率を変化させることで、初期には広く探索しながら学習が進むと自然と活用を増やす仕組みを実現できる。こうした適応的な手法は、実環境において調整が難しいハイパーパラメータを減らすという利点がある。

探索と活用のバランスを取る代表的な手法として、\(\varepsilon\)-greedy法が挙げられる。この方法では、確率 \(\varepsilon\) でランダムな行動を選び、残りの \(1-\varepsilon\) では現在の最良方策に従って行動する。\(\varepsilon\) を徐々に小さくしていけば、学習初期には広く探索し、後期には得られた知識を活用できる。

別のアプローチとしてソフトマックス方策があり、行動価値に基づいて確率的に行動を選択する。価値が高い行動ほど選ばれやすいが、確率分布を用いるため常にわずかながら探索が行われる。適切な温度パラメータを設定することで、探索量を細かく制御できる点が利点だ。

学習アルゴリズムは大きくモデルベースとモデルフリーに分けられる。モデルベースでは環境の遷移モデルを推定し、そのモデルを使って計画を立てる。一方、モデルフリーでは遷移モデルを仮定せず、経験のみに基づいて方策を更新する。どちらの手法にも利点と欠点があり、状況に応じて選択することが求められる。
また、サンプル効率を高めるために経験再生やモデル予測制御といったテクニックが用いられる。経験再生では、過去の経験をランダムに再利用することでデータの相関を減らし、学習を安定させる。一方、モデル予測制御では、学習した環境モデルを用いて将来の報酬を予測し、より計画的な行動選択を行う。
### 1.2.2 学習と評価
行動の結果得られたデータから、エージェントは方策や価値関数を更新する。価値ベースの手法では、状態価値や行動価値を推定し、その値を高める方向に方策を改善する。方策ベースの手法では、方策を直接パラメータ化し、期待報酬を最大化するようパラメータを調整する。実際には両者を組み合わせたアクター・クリティック法も広く利用されている。

学習の進度を測るには、エピソード当たりの総報酬や平均報酬などの指標を用いる。評価を定期的に行うことで、方策の改善がどの程度進んでいるかを確認できる。もし報酬が伸び悩んでいるなら、探索率や学習率といったハイパーパラメータを調整して再学習を試みることが多い。

また、評価は訓練環境とは独立したテスト環境で行うのが望ましい。訓練環境だけで評価すると、環境に過剰適合してしまい、未知の環境に対してうまく振る舞えない可能性がある。テスト環境でのパフォーマンスを確認し、一般化能力を高めることも重要である。

学習と評価を繰り返す過程で、エージェントは徐々に複雑な課題にも対応できるようになる。本章で紹介した基礎的な概念を理解し、次章以降でより詳細な理論やアルゴリズムを学んでいこう。
さらに、現実世界ではエージェントの試行回数に制限があるため、取得したデータを活用するオフライン学習も重要になる。ログデータを用いた方策評価や方策改善の手法は、実環境での試行が難しい場合に大きな効果を発揮する。
評価でよく使われる指標は次の通りだ。
- エピソード総報酬
- 平均ステップ数
- 成功率
- 学習に要した時間
指標の選択はタスクの特性や目標によって変わる。
さらに、シミュレーション環境だけでなく実機を使った評価を行う場合、センサー精度や通信遅延など実環境特有の要因が学習性能に影響を及ぼす。実機評価では安全性も重要視されるため、探索行動を抑える工夫や、失敗時に備えたフェイルセーフ機構の導入が欠かせない。
オフライン評価では、収集したデータを学習用と検証用に分けるクロスバリデーションがよく用いられる。これにより、過学習を抑えつつ方策の汎化性能を測ることができる。
実際のタスクでは、学習の安定性や収束の速さが重要視されることも多い。そのため、単に最終的な報酬だけでなく、途中の学習曲線を可視化してアルゴリズムの挙動を確認することが推奨される。学習過程を詳細に記録しておけば、後からハイパーパラメータを調整する際の貴重な手がかりとなる。
計算資源が豊富な場合、複数のパラメータ設定を並列に試すことで効率的な探索が可能となる。
最後に、本章で紹介した概念は強化学習全体の基礎を成している。エージェントと環境の相互作用を理解し、報酬を通じて学習を進める枠組みを押さえておくことで、後の章で扱う高度なアルゴリズムも理解しやすくなるだろう。
次章からは、マルコフ決定過程や動的計画法といった理論的な土台を掘り下げ、実装へ向けた知識を順を追って学んでいく。
評価指標としては総報酬のほか、エピソードの成功率や学習速度など、目的に応じたさまざまな尺度が用いられる。研究論文ではこれらの指標を比較し、アルゴリズムの優劣を検証することが一般的だ。
強化学習はまだ研究途上の分野であり、サンプル効率や安全性、説明可能性など多くの課題が残されている。最新の研究動向を追いながら、自らの問題設定に適した手法を選択することが重要だ。
現代の応用では、複雑な状態空間を扱うために関数近似が必須となっている。ニューラルネットワークや決定木など、さまざまなモデルが価値関数や方策の表現に利用されている。
近年の研究では、模倣学習や逆強化学習といった手法を併用することで、少ない試行回数でも人間に近い振る舞いを実現する試みが進んでいる。
理論と実践の両面から強化学習を学ぶことで、より効果的なアルゴリズムを開発できるだろう。
今後は実装演習を通して、本章で触れた概念を具体的に体験していこう。小規模な環境から始め、徐々に複雑な課題へ挑戦することで、強化学習の理解が深まるはずだ。
この本を通して、読者が自らのアイデアを強化学習に応用できる力を身につけることを願っている。
実装にはPythonの強化学習ライブラリが役立つ。特にOpenAI Gymをはじめとする環境構築ツールや、各種アルゴリズムを備えたフレームワークが多く公開されている。これらを活用すれば、理論を試しながら学びを深められる。
演習を通して得た知識は、研究開発や実務での応用に直結するだろう。
再現性を確保するためには、乱数シードの固定や実験設定の記録が欠かせない。実験条件を詳細に残すことで、他の研究者が結果を確認しやすくなり、自身の理解も深まる。
オープンなデータセットや公開コードを活用することで、学習結果を比較しやすくなる点も覚えておこう。
強化学習のアルゴリズムは多岐にわたるため、目的に応じて適切な手法を選ぶことが重要だ。単純な問題なら表形式のQ学習で十分だが、状態や行動が連続的な場合は関数近似を用いた方法が必要になる。
本章を通じて、強化学習の枠組みや基本概念を俯瞰してきた。これらの知識を土台に、以降の章ではより具体的なアルゴリズムとその理論的背景を学んでいく。
実際の応用では、環境にノイズがある場合や遷移確率が非定常な場合も多い。そのような状況では、環境を逐次推定しながら方策を更新するオンライン学習が効果的だ。
学習が進むにつれエージェントが環境を変化させてしまうこともあるため、継続的な評価と方策の見直しが欠かせない。
各章の内容をしっかり理解し、手を動かしながら読み進めることで、強化学習の実力が着実に身につくだろう。
ここで学んだ基礎を踏まえて、次章以降では具体的な数式展開やアルゴリズムの実装例を見ていく。自身でコードを書いて試すことが、理解への近道となるだろう。
本章はこれで終わりだが、繰り返し読み返して概念を定着させてほしい。
アルゴリズムの選択だけでなく、報酬設計や環境のモデリングも性能に大きな影響を及ぼす。自分で環境を作成する際は、学びやすさと現実性のバランスを意識しよう。
強化学習は多様な分野で応用が進む一方、理論と実装の両面で奥深いテーマが数多く存在する。計算効率や学習の安定化、倫理的配慮など、解決すべき課題は尽きない。
読者が本書を通じて強化学習の面白さを感じ、さらなる探究を続けるきっかけになることを願っている。
参考文献やオンライン資料を積極的に参照し、自分の理解を深める姿勢が大切だ。
実験と理論の両輪で学ぶことにより、強化学習を扱う力が着実に養われるだろう。
以上で第1章を終える。
