# 強化学習入門 目次

## 第1章 強化学習の基礎
### 1.1 強化学習とは
#### 1.1.1 エージェントと環境
#### 1.1.2 状態・行動・報酬
### 1.2 学習の流れ
#### 1.2.1 探索と活用
#### 1.2.2 学習と評価

## 第2章 マルコフ決定過程
### 2.1 MDPの定義
#### 2.1.1 状態遷移確率
#### 2.1.2 割引率と報酬
### 2.2 ベルマン方程式
#### 2.2.1 状態価値関数
#### 2.2.2 行動価値関数

## 第3章 動的計画法
### 3.1 方策評価
#### 3.1.1 反復方策評価
#### 3.1.2 誤差と収束
### 3.2 方策反復
#### 3.2.1 方策改善
#### 3.2.2 方策評価との繰り返し
### 3.3 価値反復
#### 3.3.1 バックアップ操作
#### 3.3.2 収束性の理解

## 第4章 モンテカルロ法
### 4.1 エピソードによる学習
#### 4.1.1 初期探索
#### 4.1.2 末尾報酬の利用
### 4.2 オンポリシーとオフポリシー
#### 4.2.1 行動方策と評価方策
#### 4.2.2 重要度サンプリング

## 第5章 時間差分学習
### 5.1 TD(0)
#### 5.1.1 TD誤差
#### 5.1.2 バックアップの特徴
### 5.2 SARSA
#### 5.2.1 オンポリシー制御
#### 5.2.2 ε-greedy
### 5.3 Q学習
#### 5.3.1 オフポリシー制御
#### 5.3.2 最大行動価値の利用

## 第6章 方策近似
### 6.1 関数近似
#### 6.1.1 特徴量設計
#### 6.1.2 線形近似法
### 6.2 深層強化学習概観
#### 6.2.1 DQN
#### 6.2.2 ポリシーグラディエント法

## 第7章 実践編
### 7.1 環境の構築
#### 7.1.1 OpenAI Gymの導入
#### 7.1.2 自作環境の作成
### 7.2 シミュレーションと評価
#### 7.2.1 学習曲線の可視化
#### 7.2.2 ハイパーパラメータ調整
### 7.3 応用事例
#### 7.3.1 ゲームAI
#### 7.3.2 ロボット制御
